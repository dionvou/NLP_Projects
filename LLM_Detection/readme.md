# TASK
We try to create a dataset in order to predict whether a text is generated by Large Language Model.

## Dataset
Our dataset was created using several different LLM and also student essays.
* train_essays.csv: Data from kaggle competition link: https://www.kaggle.com/competitions/llm-detect-ai-generated-text \

 * id : essay id
 * prompt_id : id of the prompt (topic) that the student wrire the essay
 * text : The essay
 * generated : 0 if it was written by student or 1 if written by LLM
* prompt_1.csv: Data generated using chatgpt tubro 3.5 with prompt_id = 1
 * same columns as train_essays.csv
* prompt_2.csv: Data generated using chatgpt tubro 3.5 with prompt_id = 2
 * same columns as train_essays.csv
* data_new_davinci.csv: Data generated using chagpt davinci 003.
 * same columns as train_essays.csv
* bard_data.csv: Data generated using bard
 * same columns as train_essays.csv
* train_v2_drcat_02.csv: Data extracted from https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset
 * text: The essay
 * label: 0 if it was written by student or 1 if written by LLM (equal to generated of train_essays.csv)
 * prompt_name: The name of the prompt(15 different prompts)
 * source: the source of the essay
 
## Process
We used as training set all data except train_v2_drcat_02.csv. We kept train_v2_drcat_02.csv as test set.
Our test set contains several different prompts, which made our problem more difficult. If we keep only the prompts we have in the training set, the scores are almost perfect. But in order to evaluate our model to general texts we need to have a more general test set. 

* Firsty, we use Tfidf vectorization witn ngram(3,5). We pick MultinomialNB() as our model to train. The results show that recall score of generated texts is low. But precision is close to 1, which could be good, depending on what we would like to achieve.
* Secondly, we tried to explore the similarity of each generated text, with each student text. We see that most of our generated texts are not very similar with the student ones. So we decided to remove text with low similarity and high probality, and mixed the with student text. In that way we can make more similar text. That wish that we will help our model to distinguish better the marginal texts. We had little improvement in recall, keeping accuracy and f1 the same.
* Thirdly, we use k-means in order to explore the balance between categories. We add some more data using Palm, to achieve recall 0.88 with 0.90 accuracy.

